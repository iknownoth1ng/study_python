{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "协程(coroutine)\n",
    "- 实现并发编程的一种方式。多线程/多进程，是解决并发问题的经典模型之一。在最初的互联网世界，起到举足轻重的作用。\n",
    "- Python2中，使用生成器来实现Python协程。例如以下代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PRODUCER] Producing 1...\n",
      "[CONSUMER] Consuming 1...\n",
      "[PRODUCER] Consumer return: 200 OK\n",
      "[PRODUCER] Producing 2...\n",
      "[CONSUMER] Consuming 2...\n",
      "[PRODUCER] Consumer return: 200 OK\n",
      "[PRODUCER] Producing 3...\n",
      "[CONSUMER] Consuming 3...\n",
      "[PRODUCER] Consumer return: 200 OK\n",
      "[PRODUCER] Producing 4...\n",
      "[CONSUMER] Consuming 4...\n",
      "[PRODUCER] Consumer return: 200 OK\n",
      "[PRODUCER] Producing 5...\n",
      "[CONSUMER] Consuming 5...\n",
      "[PRODUCER] Consumer return: 200 OK\n"
     ]
    }
   ],
   "source": [
    "# Python对协程的支持是通过generator实现的。\n",
    "\n",
    "# 在generator中，我们不但可以通过for循环来迭代，还可以不断调用next()函数获取由yield语句返回的下一个值。\n",
    "\n",
    "# 但是Python的yield不但可以返回一个值，它还可以接收调用者发出的参数。\n",
    "\n",
    "# 来看例子：\n",
    "\n",
    "# 传统的生产者-消费者模型是一个线程写消息，一个线程取消息，通过锁机制控制队列和等待，但一不小心就可能死锁。\n",
    "\n",
    "# 如果改用协程，生产者生产消息后，直接通过yield跳转到消费者开始执行，待消费者执行完毕后，切换回生产者继续生产，效率极高：\n",
    "\n",
    "def consumer():\n",
    "    r = ''\n",
    "    while True:\n",
    "        n = yield r\n",
    "        if not n:\n",
    "            return\n",
    "        print(f'[CONSUMER] Consuming {n}...')\n",
    "        r = '200 OK'\n",
    "\n",
    "\n",
    "def produce(c):\n",
    "    c.send(None)\n",
    "    for n in range(1, 6):\n",
    "        print(f'[PRODUCER] Producing {n}...')\n",
    "        r = c.send(n)\n",
    "        print(f'[PRODUCER] Consumer return: {r}')\n",
    "    c.close()\n",
    "\n",
    "\n",
    "c = consumer()\n",
    "produce(c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python 3.7以后"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crawling url_1\n",
      "OK url_1\n",
      "crawling url_2\n",
      "OK url_2\n",
      "crawling url_3\n",
      "OK url_3\n",
      "crawling url_4\n",
      "OK url_4\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 10 s\n"
     ]
    }
   ],
   "source": [
    "# 先看一个例子\n",
    "import time\n",
    "\n",
    "\n",
    "def crawl_page(url):\n",
    "    print(f'crawling {url}')\n",
    "    sleep_time = int(url.split('_')[-1])\n",
    "    time.sleep(sleep_time)\n",
    "    print(f'OK {url}')\n",
    "\n",
    "\n",
    "def main(urls):\n",
    "    for url in urls:\n",
    "        crawl_page(url)\n",
    "\n",
    "\n",
    "%time main(['url_1', 'url_2', 'url_3', 'url_4'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crawling url_1\n",
      "OK url_1\n",
      "crawling url_2\n",
      "OK url_2\n",
      "crawling url_3\n",
      "OK url_3\n",
      "crawling url_4\n",
      "OK url_4\n"
     ]
    }
   ],
   "source": [
    "# 引入协程\n",
    "import asyncio\n",
    "\n",
    "\n",
    "async def crawl_page(url):\n",
    "    print(f'crawling {url}')\n",
    "    sleep_time = int(url.split('_')[-1])\n",
    "    await asyncio.sleep(sleep_time)\n",
    "    print(f'OK {url}')\n",
    "\n",
    "\n",
    "async def main(urls):\n",
    "    for url in urls:\n",
    "        await crawl_page(url)\n",
    "\n",
    "await(main(['url_1', 'url_2', 'url_3', 'url_4']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还是10秒,优化，引入任务(Task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crawling url_1\n",
      "crawling url_2\n",
      "crawling url_3\n",
      "crawling url_4\n",
      "OK url_1\n",
      "OK url_2\n",
      "OK url_3\n",
      "OK url_4\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "async def crawl_page(url):\n",
    "    print(f'crawling {url}')\n",
    "    sleep_time = int(url.split('_')[-1])\n",
    "    await asyncio.sleep(sleep_time)\n",
    "    print(f'OK {url}')\n",
    "\n",
    "\n",
    "async def main(urls):\n",
    "    tasks = [asyncio.create_task(crawl_page(url)) for url in urls]\n",
    "    for task in tasks:\n",
    "        await task\n",
    "\n",
    "await(main(['url_1', 'url_2', 'url_3', 'url_4']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "执行tasks，另一种做法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crawling url_1\n",
      "crawling url_2\n",
      "crawling url_3\n",
      "crawling url_4\n",
      "OK url_1\n",
      "OK url_2\n",
      "OK url_3\n",
      "OK url_4\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "async def crawl_page(url):\n",
    "    print(f'crawling {url}')\n",
    "    sleep_time = int(url.split('_')[-1])\n",
    "    await asyncio.sleep(sleep_time)\n",
    "    print(f'OK {url}')\n",
    "\n",
    "\n",
    "async def main(urls):\n",
    "    tasks = [asyncio.create_task(crawl_page(url)) for url in urls]\n",
    "    await asyncio.gather(*tasks)\n",
    "\n",
    "await(main(['url_1', 'url_2', 'url_3', 'url_4']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解密协程运行时"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before await\n",
      "worker_1 start\n",
      "worker_1 done\n",
      "awaited worker_1\n",
      "worker_2 start\n",
      "worker_2 done\n",
      "awaited worker_2\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "async def worker_1():\n",
    "    print('worker_1 start')\n",
    "    await asyncio.sleep(1)\n",
    "    print('worker_1 done')\n",
    "\n",
    "\n",
    "async def worker_2():\n",
    "    print('worker_2 start')\n",
    "    await asyncio.sleep(2)\n",
    "    print('worker_2 done')\n",
    "\n",
    "\n",
    "async def main():\n",
    "    print('before await')\n",
    "    await worker_1()\n",
    "    print('awaited worker_1')\n",
    "    await worker_2()\n",
    "    print('awaited worker_2')\n",
    "\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before await\n",
      "worker_1 start\n",
      "worker_2 start\n",
      "worker_1 done\n",
      "awaited worker_1\n",
      "worker_2 done\n",
      "awaited worker_2\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "async def worker_1():\n",
    "    print('worker_1 start')\n",
    "    await asyncio.sleep(1)\n",
    "    print('worker_1 done')\n",
    "\n",
    "\n",
    "async def worker_2():\n",
    "    print('worker_2 start')\n",
    "    await asyncio.sleep(2)\n",
    "    print('worker_2 done')\n",
    "\n",
    "\n",
    "async def main():\n",
    "    task1 = asyncio.create_task(worker_1())\n",
    "    task2 = asyncio.create_task(worker_2())\n",
    "    print('before await')\n",
    "    await task1\n",
    "    print('awaited worker_1')\n",
    "    await task2\n",
    "    print('awaited worker_2')\n",
    "\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, ZeroDivisionError('division by zero'), CancelledError()]\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "async def worker_1():\n",
    "    await asyncio.sleep(1)\n",
    "    return 1\n",
    "\n",
    "\n",
    "async def worker_2():\n",
    "    await asyncio.sleep(2)\n",
    "    return 2/0\n",
    "\n",
    "\n",
    "async def worker_3():\n",
    "    await asyncio.sleep(3)\n",
    "    return 3\n",
    "\n",
    "\n",
    "async def main():\n",
    "    task_1 = asyncio.create_task(worker_1())\n",
    "    task_2 = asyncio.create_task(worker_2())\n",
    "    task_3 = asyncio.create_task(worker_3())\n",
    "\n",
    "    await asyncio.sleep(2)\n",
    "    task_3.cancel()\n",
    "\n",
    "    res = await asyncio.gather(task_1, task_2, task_3, return_exceptions=True)\n",
    "    print(res)\n",
    "\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "协程实现一个生产者消费者模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "producer_1 set a val: 7\n",
      "producer_2 set a val: 3\n",
      "consumer_1 get a val: 7\n",
      "consumer_2 get a val: 3\n",
      "producer_1 set a val: 4\n",
      "producer_2 set a val: 6\n",
      "consumer_2 get a val: 4\n",
      "consumer_1 get a val: 6\n",
      "producer_1 set a val: 9\n",
      "producer_2 set a val: 10\n",
      "consumer_1 get a val: 9\n",
      "consumer_2 get a val: 10\n",
      "producer_1 set a val: 8\n",
      "producer_2 set a val: 6\n",
      "consumer_2 get a val: 8\n",
      "consumer_1 get a val: 6\n",
      "producer_1 set a val: 5\n",
      "producer_2 set a val: 5\n",
      "consumer_1 get a val: 5\n",
      "consumer_2 get a val: 5\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import asyncio\n",
    "\n",
    "\n",
    "async def consumer(queue, id):\n",
    "    while True:\n",
    "        val = await queue.get()\n",
    "        print(f'{id} get a val: {val}')\n",
    "        await asyncio.sleep(1)\n",
    "\n",
    "\n",
    "async def producer(queue, id):\n",
    "    for _ in range(5):\n",
    "        val = random.randint(1, 10)\n",
    "        await queue.put(val)\n",
    "        print(f'{id} set a val: {val}')\n",
    "        await asyncio.sleep(1)\n",
    "\n",
    "\n",
    "async def main():\n",
    "    queue = asyncio.Queue()\n",
    "\n",
    "    consumer_1 = asyncio.create_task(consumer(queue, 'consumer_1'))\n",
    "    consumer_2 = asyncio.create_task(consumer(queue, 'consumer_2'))\n",
    "\n",
    "    producer_1 = asyncio.create_task(producer(queue, 'producer_1'))\n",
    "    producer_2 = asyncio.create_task(producer(queue, 'producer_2'))\n",
    "\n",
    "    await asyncio.sleep(10)\n",
    "    consumer_1.cancel()\n",
    "    consumer_2.cancel()\n",
    "\n",
    "    await asyncio.gather(consumer_1, consumer_2, producer_1, producer_2, return_exceptions=True)\n",
    "\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实战：豆瓣今日推荐电影爬虫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 同步版本\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def main():\n",
    "    url = \"https://movie.douban.com/cinema/later/beijing/\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36 QIHU 360SE'\n",
    "    }\n",
    "    init_page = requests.get(url, headers=headers).content\n",
    "    init_soup = BeautifulSoup(init_page, \"lxml\")\n",
    "\n",
    "    all_movies = init_soup.find(\"div\", id=\"showing-soon\")\n",
    "    for each_movie in all_movies.find_all(\"div\", class_=\"item\"):\n",
    "        all_a_tag = each_movie.find_all(\"a\")\n",
    "        all_li_tag = each_movie.find_all(\"li\")\n",
    "\n",
    "        movie_name = all_a_tag[1].text\n",
    "        url_to_fetch = all_a_tag[0]['href']\n",
    "        movie_date = all_li_tag[0].text\n",
    "\n",
    "        response_item = requests.get(url_to_fetch, headers=headers).content\n",
    "        soup_item = BeautifulSoup(response_item, \"lxml\")\n",
    "        img_tag = soup_item.find(\"img\")\n",
    "\n",
    "        print(f'{movie_name} {movie_date} {img_tag.get(\"src\")}')\n",
    "\n",
    "\n",
    "%time main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 协程版本\n",
    "import asyncio\n",
    "import aiohttp\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "async def fetch_content(url):\n",
    "    header = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.66 Safari/537.36'\n",
    "    }\n",
    "    async with aiohttp.ClientSession(headers=header, connector=aiohttp.TCPConnector(verify_ssl=False)) as session:\n",
    "        async with session.get(url) as resp:\n",
    "            return await resp.text()\n",
    "\n",
    "\n",
    "async def main():\n",
    "    url = \"https://movie.douban.com/cinema/later/beijing/\"\n",
    "\n",
    "    init_page = await fetch_content(url)\n",
    "    init_soup = BeautifulSoup(init_page, \"lxml\")\n",
    "\n",
    "    movie_names, urls_to_fetch, movie_dates = [], [], []\n",
    "\n",
    "    all_movies = init_soup.find(\"div\", id=\"showing-soon\")\n",
    "    for each_movie in all_movies.find_all(\"div\", class_=\"item\"):\n",
    "        all_a_tag = each_movie.find_all(\"a\")\n",
    "        all_li_tag = each_movie.find_all(\"li\")\n",
    "\n",
    "        movie_names.append(all_a_tag[1].text)\n",
    "        urls_to_fetch.append(all_a_tag[0]['href'])\n",
    "        movie_dates.append(all_li_tag[0].text)\n",
    "\n",
    "    tasks = [fetch_content(url) for url in urls_to_fetch]\n",
    "    pages = await asyncio.gather(*tasks)\n",
    "\n",
    "    for movie_name, movie_date, page in zip(movie_names, movie_dates, pages):\n",
    "        soup_item = BeautifulSoup(page, 'lxml')\n",
    "        img_tag = soup_item.find(\"img\")\n",
    "\n",
    "        print(f'{movie_name} {movie_date} {img_tag.get(\"src\")}')\n",
    "\n",
    "await main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af2924f10f5eb76fd65ccea448a69e80be7838e947e8a1ae982ab7073141c8a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
